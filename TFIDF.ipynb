{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from datetime import datetime\n",
    "import nlpaug.augmenter.word as naw\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.combine import SMOTETomek\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\Users\\buket\\Desktop\\THESISDOCUMENTS\\Twoclasses_withNLP\\Bug_dataset.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priority Column Preprocessing\n",
    "##### Based on the approach Mapping should be changed and data = data[data['priority'] != 'P3'] should be dropped or kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping logic\n",
    "data = data[data['priority'] != '--'] #always drop '--'\n",
    "data = data[data['priority'] != 'P3'] # Depending on the approach keep or erase\n",
    "priority_mapping = { #Depending on the approach change mapping\n",
    "    'P1': 0,  \n",
    "    'P2': 0,  \n",
    "    'P4': 1,  \n",
    "    'P5': 2   \n",
    "}\n",
    "\n",
    "\n",
    "data['priority'] = data['priority'].map(priority_mapping)\n",
    "\n",
    "print(data['priority'].value_counts())\n",
    "\n",
    "unique_count = data['priority'].nunique()\n",
    "print(f\"Number of unique values: {unique_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing and Categorical Values in Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['severity'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['severity'].value_counts(dropna=False))\n",
    "\n",
    "# Drop rows with NaN values in the 'severity' column\n",
    "data = data.dropna(subset=['severity'])\n",
    "\n",
    "print(data['severity'].value_counts())\n",
    "print(f\"Updated dataset shape: {data.shape}\")\n",
    "\n",
    "# Severity mapping based on the website\n",
    "severity_mapping = {\n",
    "    'S2': 2,       # Major\n",
    "    'S1': 1,       # Critical\n",
    "    'S3': 3,       # Normal\n",
    "    'S4': 4,       # Minor\n",
    "    'normal': 3,   # Normal\n",
    "    'major': 2,    # Major\n",
    "    'minor': 4,    # Minor\n",
    "    'critical': 1, # Critical\n",
    "    'trivial': 4,  # Minor\n",
    "    'blocker': 1   # Critical\n",
    "}\n",
    "\n",
    "data['severity_mapped'] = data['severity'].map(severity_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[['severity', 'severity_mapped']].head())\n",
    "\n",
    "print(data['severity_mapped'].unique())\n",
    "print(data['severity_mapped'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting and Extracting Features from Creation Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'creation_time' to datetime\n",
    "data['creation_time'] = pd.to_datetime(data['creation_time'])\n",
    "\n",
    "print(data['creation_time'].head())\n",
    "print(data['creation_time'].dtypes)\n",
    "\n",
    "# Get the maximum creation time from the dataset as the reference date\n",
    "reference_date = data['creation_time'].max()\n",
    "print(f\"Reference date: {reference_date}\")\n",
    "\n",
    "# Calculate bug age in days\n",
    "data['bug_age'] = (reference_date - data['creation_time']).dt.days\n",
    "print(data[['creation_time', 'bug_age']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Product Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_counts = data['product'].value_counts()\n",
    "\n",
    "# Define the threshold for grouping smaller categories\n",
    "threshold = 100\n",
    "data['product'] = data['product'].apply(lambda x: x if product_counts[x] >= threshold else 'Other')\n",
    "\n",
    "# Recalculate value counts after grouping smaller categories\n",
    "updated_product_counts = data['product'].value_counts()\n",
    "\n",
    "# Percentage of bugs grouped under \"Other\"\n",
    "other_percentage = (updated_product_counts['Other'] / len(data)) * 100\n",
    "\n",
    "updated_product_counts, other_percentage\n",
    "\n",
    "unique_count = data['product'].nunique()\n",
    "print(f\"Number of unique values: {unique_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Textual Columns\n",
    "##### Not dropping the missing values in the description column here since they will be merged with the summary column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Missing values in 'description': {data['description'].isnull().sum()}\")\n",
    "data['description'] = data['description'].fillna('')\n",
    "print(f\"Missing values in 'description': {data['description'].isnull().sum()}\")\n",
    "\n",
    "# Merge the 'summary' and 'description' columns\n",
    "data['merged_summary_description'] = data['summary'] + \" \" + data['description']\n",
    "\n",
    "# Verify the new column\n",
    "print(data[['summary', 'description', 'merged_summary_description']].head())\n",
    "\n",
    "print(f\"Missing values in 'merged_summary_description': {data['merged_summary_description'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop multiple columns\n",
    "data = data.drop(['summary', 'creation_time', 'id', 'description', 'severity'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning: Replacing Contractions and Removing Special Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Replacing Contractions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern with replacements\n",
    "def replace_contractions(text):\n",
    "    contractions = {\n",
    "        r\"don´t\": \"do not\",\n",
    "        r\"isn´t\": \"is not\",\n",
    "        r\"hasn´t\": \"has not\",\n",
    "        r\"doesn´t\": \"does not\",\n",
    "        r\"haven´t\": \"have not\",\n",
    "        r\"aren´t\": \"are not\",\n",
    "        r\"couldn´t\": \"could not\",\n",
    "        r\"can´t\": \"can not\"\n",
    "    }\n",
    "    # Compile a regex pattern\n",
    "    pattern = re.compile(\"|\".join(contractions.keys()), flags=re.IGNORECASE)\n",
    "    return pattern.sub(lambda x: contractions[x.group().lower()], text)\n",
    "\n",
    "# Apply the function to the DataFrame column\n",
    "data['merged_summary_description'] = data['merged_summary_description'].apply(replace_contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Cleaning Special Characters and Numbers:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_special_characters_and_numbers(text):\n",
    "    \"\"\"\n",
    "    Replace all special characters and numbers in a text with a blank space.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text with special characters and numbers replaced by spaces.\n",
    "    \"\"\"\n",
    "    # Remove all special characters and numbers\n",
    "    text = re.sub(r'[^A-Za-z\\s]', ' ', text)  # Keep only alphabetic characters and spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Replace multiple spaces with a single space\n",
    "    return text\n",
    "\n",
    "\n",
    "def _clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean individual text by removing special characters, numbers, and ensuring it is a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    # Ensure the input is a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    return clean_special_characters_and_numbers(text)\n",
    "\n",
    "\n",
    "def preprocessor(text_or_series):\n",
    "    \"\"\"\n",
    "    Preprocess text or pandas Series by cleaning special characters and numbers.\n",
    "\n",
    "    Args:\n",
    "        text_or_series (str or pd.Series): Input text or pandas Series.\n",
    "\n",
    "    Returns:\n",
    "        str or pd.Series: Cleaned text or Series.\n",
    "    \"\"\"\n",
    "    if isinstance(text_or_series, pd.Series):\n",
    "        # Apply the preprocessor to each element of the Series\n",
    "        return text_or_series.apply(_clean_text)\n",
    "    else:\n",
    "        # Otherwise, treat it as a single text input\n",
    "        return _clean_text(text_or_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed = data\n",
    "data_preprocessed['merged_summary_description'] = preprocessor(data['merged_summary_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed['merged_summary_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_preprocessed['priority'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = data_preprocessed.drop(['priority'], axis=1)  \n",
    "y = data_preprocessed['priority']  \n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Output shapes for verification\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying NLP augmentation only to train data\n",
    "##### This section should be commented out based on the approach - without NLP augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_for_NLP = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_for_NLP['priority'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def augment_minority_class(df, text_column, label_column, aug_n=1):\n",
    "    \"\"\"\n",
    "    Augment text samples of the minority class using synonyms.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataset with text and labels.\n",
    "        text_column (str): Name of the column containing text data.\n",
    "        label_column (str): Name of the column containing class labels.\n",
    "        aug_n (int): Number of augmented samples to create per row.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Original dataset appended with augmented samples.\n",
    "    \"\"\"\n",
    "    # Identify the minority class\n",
    "    minority_class = df[label_column].value_counts().idxmin()\n",
    "\n",
    "    # Filter rows belonging to the minority class\n",
    "    minority_class_rows = df[df[label_column] == minority_class]\n",
    "\n",
    "    # Initialize synonym augmenter\n",
    "    synonym_aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "    augmented_rows = []\n",
    "\n",
    "    def dynamic_synonym_augmentation(sentence, n=1):\n",
    "        \"\"\"\n",
    "        Perform synonym-based augmentation with dynamic word count.\n",
    "\n",
    "        Args:\n",
    "            sentence (str): Input text to augment.\n",
    "            n (int): Number of augmented samples to generate.\n",
    "\n",
    "        Returns:\n",
    "            list: List of augmented text samples.\n",
    "        \"\"\"\n",
    "        if not sentence or not isinstance(sentence, str):\n",
    "            return [sentence] * n  # Return the original sentence if invalid\n",
    "\n",
    "        # Tokenize the sentence into words\n",
    "        words = sentence.split()\n",
    "        num_words = len(words)\n",
    "\n",
    "        # Dynamically set the maximum number of words to augment\n",
    "        if num_words <= 3:\n",
    "            aug_max = 1  # Augment at most 1 word for short texts\n",
    "        elif num_words <= 20:\n",
    "            aug_max = max(1, int(0.2 * num_words))  # Augment 20% of words for medium-length texts\n",
    "        else:\n",
    "            aug_max = max(1, int(0.1 * num_words))  # Augment 10% of words for long texts\n",
    "\n",
    "        # Update the augmenter with dynamic `aug_max`\n",
    "        synonym_aug.aug_max = aug_max\n",
    "\n",
    "        # Generate augmented versions\n",
    "        augmented_sentences = synonym_aug.augment(sentence, n=n)\n",
    "        return augmented_sentences\n",
    "\n",
    "    # Loop through each row in the minority class\n",
    "    for i in minority_class_rows.index:\n",
    "        original_row = df.loc[i].copy()  # Get the original row as a Series\n",
    "\n",
    "        # Augment the text column\n",
    "        original_text = original_row[text_column]\n",
    "        augmented_versions = dynamic_synonym_augmentation(original_text, n=aug_n)\n",
    "\n",
    "        # Create new rows for each augmented version\n",
    "        for aug_text in augmented_versions:\n",
    "            augmented_row = original_row.copy()  # Copy the original row\n",
    "            augmented_row[text_column] = aug_text  # Replace only the text column\n",
    "            augmented_rows.append(augmented_row)\n",
    "\n",
    "    # Convert augmented rows to a DataFrame\n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "\n",
    "    # Combine original data with augmented data\n",
    "    combined_df = pd.concat([df, augmented_df], ignore_index=True)\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_for_NLP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Augmented_Train_Data = augment_minority_class(train_data_for_NLP, text_column='merged_summary_description', label_column='priority', aug_n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Augmented_Train_Data['priority'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Augmented_Train_Data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Augmented_Train_Data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train_Augmented = Augmented_Train_Data.drop(['priority'], axis=1)\n",
    "y_Train_Augmented = Augmented_Train_Data['priority']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train_Augmented.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train_Augmented['merged_summary_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_Train_Augmented.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_Train_Augmented.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "stop_words=set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_lemmetizer(text):\n",
    "\n",
    "    text=tokenizer.tokenize(text)\n",
    "    text= [token for token in text if token not in stop_words]\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train_Augmented['merged_summary_description'] = X_Train_Augmented['merged_summary_description'].apply(tokenizer_lemmetizer)\n",
    "X_test['merged_summary_description'] = X_test['merged_summary_description'].apply(tokenizer_lemmetizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_Train_Augmented\n",
    "X_test=X_test\n",
    "y_train = y_Train_Augmented\n",
    "y_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline with TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.Model-Based Methods for Addressing Class Imbalance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"results_tfidf_none\", exist_ok=True)  # Create a folder to save results\n",
    "\n",
    "\n",
    "# Features\n",
    "text_feature = 'merged_summary_description'\n",
    "numeric_features = ['bug_age', 'severity_mapped']\n",
    "categorical_features = ['product']\n",
    "\n",
    "# Define transformers\n",
    "text_transformer = TfidfVectorizer(max_features=10000, stop_words='english', ngram_range=(1, 2), max_df=0.8)\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Combine transformers in a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', text_transformer, text_feature),\n",
    "        ('numeric', numeric_transformer, numeric_features),\n",
    "        ('categorical', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "classifiers = {\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "        'param_grid': {\n",
    "            'classifier__n_estimators': [100, 300],  \n",
    "            'classifier__max_depth': [None, 10],  \n",
    "            'classifier__min_samples_split': [5, 10], \n",
    "            'classifier__min_samples_leaf': [1, 4],  \n",
    "            'classifier__max_features': ['sqrt'],  \n",
    "            'classifier__bootstrap': [True],  \n",
    "            'classifier__criterion': ['gini', 'entropy']  \n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(eval_metric=\"mlogloss\", random_state=42, \n",
    "                               objective='multi:softmax', num_class=3),\n",
    "        'param_grid': {\n",
    "            'classifier__n_estimators': [100, 300],  \n",
    "            'classifier__max_depth': [6, 15],  \n",
    "            'classifier__learning_rate': [0.01, 0.1],  \n",
    "            'classifier__subsample': [0.6, 0.8],  \n",
    "            'classifier__colsample_bytree': [0.8, 1.0], \n",
    "            'classifier__gamma': [0, 0.1, 0.2],  \n",
    "        }\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(random_state=42, class_weight='balanced', \n",
    "                                    multi_class='multinomial', max_iter=1000),\n",
    "        'param_grid': {\n",
    "            'classifier__C': [0.1, 1, 10],  \n",
    "            'classifier__penalty': ['l2'],  \n",
    "            'classifier__solver': ['lbfgs'],  \n",
    "            'classifier__max_iter': [500, 1000],  \n",
    "        }\n",
    "    }\n",
    "}\n",
    "# Loop through classifiers and perform GridSearchCV\n",
    "for name, config in classifiers.items():\n",
    "    print(f\"Training and tuning {name}...\")\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', config['model'])\n",
    "    ])\n",
    "    \n",
    "    # GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid=config['param_grid'],\n",
    "        scoring='f1',  # Binary classification friendly metric\n",
    "        cv=3,  # 3-fold cross-validation\n",
    "        verbose=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best parameters and evaluation\n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred, digits=3)\n",
    "    print(f\"\\nResults for {name} :\\n\")\n",
    "    print(report)\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "        \n",
    "    # Save the model\n",
    "    model_filename = f\"results_tfidf_none/_{name}_model.pkl\"\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(best_model, model_file)\n",
    "    print(f\"Model saved to {model_filename}\")\n",
    "        \n",
    "     # Save the classification report\n",
    "    report_filename = f\"results_tfidf_none/_{name}_report.txt\"\n",
    "    with open(report_filename, 'w') as report_file:\n",
    "        report_file.write(f\"Resampling Technique: \\n\")\n",
    "        report_file.write(f\"Classifier: {name}\\n\")\n",
    "        report_file.write(f\"Best Parameters: {grid_search.best_params_}\\n\\n\")\n",
    "        report_file.write(\"Classification Report:\\n\")\n",
    "        report_file.write(report)\n",
    "    print(f\"Classification report saved to {report_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.Resampling Techniques Applied for Adressing Class Imbalance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "os.makedirs(\"results_tfidf\", exist_ok=True)\n",
    "\n",
    "# Features\n",
    "text_feature = 'merged_summary_description'\n",
    "numeric_features = ['bug_age', 'severity_mapped']\n",
    "categorical_features = ['product']\n",
    "\n",
    "# Define transformers\n",
    "text_transformer = TfidfVectorizer(max_features=10000, stop_words='english', ngram_range=(1, 2), max_df=0.8)\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Combine transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', text_transformer, text_feature),\n",
    "        ('numeric', numeric_transformer, numeric_features),\n",
    "        ('categorical', categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "resamplers = {\n",
    "    'SMOTE': SMOTE(sampling_strategy={0: 5000, 1: 5800, 2: 7072}, k_neighbors=5, random_state=42),\n",
    "    \n",
    "    # For SMOTETomek, pass the SMOTE instance as an argument (Tomek Links is applied automatically)\n",
    "    'SMOTETomek': SMOTETomek(smote=SMOTE(sampling_strategy={0: 5000, 1: 5800, 2: 7072}, k_neighbors=5, random_state=42), random_state=42),\n",
    "    \n",
    "    # Tomek Links - no need for sampling_strategy for this since it's handled automatically\n",
    "    'Tomek Links': TomekLinks(sampling_strategy='auto'),\n",
    "}\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'param_grid': {\n",
    "            'classifier__n_estimators': [100, 300], \n",
    "            'classifier__max_depth': [None, 10],  \n",
    "            'classifier__min_samples_split': [5, 10],  \n",
    "            'classifier__min_samples_leaf': [1, 4],  \n",
    "            'classifier__max_features': ['sqrt'], \n",
    "            'classifier__bootstrap': [True],  \n",
    "            'classifier__criterion': ['gini', 'entropy'] \n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(random_state=42, objective='multi:softmax', num_class=3),\n",
    "        'param_grid': {\n",
    "            'classifier__n_estimators': [100, 300],  \n",
    "            'classifier__max_depth': [6, 15],  \n",
    "            'classifier__learning_rate': [0.01, 0.1], \n",
    "            'classifier__subsample': [0.6, 0.8], \n",
    "            'classifier__colsample_bytree': [0.8, 1.0], \n",
    "            'classifier__gamma': [0, 0.1, 0.2],  \n",
    "        }\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(random_state=42, multi_class='multinomial', max_iter=1000),\n",
    "        'param_grid': {\n",
    "            'classifier__C': [0.1, 1, 10],  \n",
    "            'classifier__penalty': ['l2'],  \n",
    "            'classifier__solver': ['lbfgs'],  \n",
    "            'classifier__max_iter': [500, 1000],  \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Loop through resampling techniques\n",
    "for resampler_name, resampler in resamplers.items():\n",
    "    print(f\"\\nUsing Resampling Technique: {resampler_name}\")\n",
    "    \n",
    "    # Display class distribution before resampling\n",
    "    print(f\"Class distribution before resampling: {Counter(y_train)}\")\n",
    "    \n",
    "    # Loop through classifiers\n",
    "    for name, config in classifiers.items():\n",
    "        print(f\"\\nTraining and tuning {name} with {resampler_name}...\")\n",
    "        \n",
    "        # Create pipeline with resampling integrated\n",
    "        pipeline = ImbPipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('resampler', resampler),\n",
    "            ('classifier', config['model']),\n",
    "        ])\n",
    "        \n",
    "        # GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grid=config['param_grid'],\n",
    "            scoring='f1_macro',\n",
    "            cv=3,\n",
    "            verbose=2,\n",
    "            n_jobs=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "         \n",
    "        # Evaluate the model\n",
    "        print(f\"Best parameters for {name} with {resampler_name}: {grid_search.best_params_}\")\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        # Classification report\n",
    "        report = classification_report(y_test, y_pred, digits=3)\n",
    "        print(f\"\\nResults for {name} with {resampler_name}:\\n\")\n",
    "        print(report)\n",
    "        \n",
    "        # Save the model\n",
    "        model_filename = f\"results_tfidf/{resampler_name}_{name}_model.pkl\"\n",
    "        with open(model_filename, 'wb') as model_file:\n",
    "            pickle.dump(best_model, model_file)\n",
    "        print(f\"Model saved to {model_filename}\")\n",
    "        \n",
    "        # Save the classification report\n",
    "        report_filename = f\"results_tfidf/{resampler_name}_{name}_report.txt\"\n",
    "        with open(report_filename, 'w') as report_file:\n",
    "            report_file.write(f\"Resampling Technique: {resampler_name}\\n\")\n",
    "            report_file.write(f\"Classifier: {name}\\n\")\n",
    "            report_file.write(f\"Best Parameters: {grid_search.best_params_}\\n\\n\")\n",
    "            report_file.write(\"Classification Report:\\n\")\n",
    "            report_file.write(report)\n",
    "        print(f\"Classification report saved to {report_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
